\documentclass{article}

\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage[usenames,dvipsnames]{color} % Required for custom colors
\usepackage{graphicx} % Required to insert images
\usepackage{listings} % Required for insertion of code
\usepackage{courier} % Required for the courier font
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{hyperref}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1} % Line spacing

% Set up the header and footer
\pagestyle{fancy}
\lhead{\hmwkAuthorName} % Top left header
\chead{\hmwkClass\ (\hmwkClassInstructor\ \hmwkClassTime): \hmwkTitle} % Top center head
\rhead{\firstxmark} % Top right header
\lfoot{\lastxmark} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage\ of\ \protect\pageref{LastPage}} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

\setlength\parindent{0pt} % Removes all indentation from paragraphs

\definecolor{MyDarkGreen}{rgb}{0.0,0.4,0.0} % This is the color used for comments
\lstloadlanguages{Perl} % Load Perl syntax for listings, for a list of other languages supported see: ftp://ftp.tex.ac.uk/tex-archive/macros/latex/contrib/listings/listings.pdf
\lstset{language=Perl, % Use Perl in this example
        frame=single, % Single frame around code
        basicstyle=\small\ttfamily, % Use small true type font
        keywordstyle=[1]\color{Blue}\bf, % Perl functions bold and blue
        keywordstyle=[2]\color{Purple}, % Perl function arguments purple
        keywordstyle=[3]\color{Blue}\underbar, % Custom functions underlined and blue
        identifierstyle=, % Nothing special about identifiers
        commentstyle=\usefont{T1}{pcr}{m}{sl}\color{MyDarkGreen}\small, % Comments small dark green courier font
        stringstyle=\color{Purple}, % Strings are purple
        showstringspaces=false, % Don't put marks in string spaces
        tabsize=5, % 5 spaces per tab
        %
        % Put standard Perl functions not included in the default language here
        morekeywords={rand},
        %
        % Put Perl function parameters here
        morekeywords=[2]{on, off, interp},
        %
        % Put user defined functions here
        morekeywords=[3]{test},
       	%
        morecomment=[l][\color{Blue}]{...}, % Line continuation (...) like blue comment
        numbers=left, % Line numbers on left
        firstnumber=1, % Line numbers start with line 1
        numberstyle=\tiny\color{Blue}, % Line numbers are blue and small
        stepnumber=5 % Line numbers go in steps of 5
}

% Creates a new command to include a perl script, the first parameter is the filename of the script (without .pl), the second parameter is the caption
\newcommand{\pythonscript}[2]{
\begin{itemize}
\item[]\lstinputlisting[caption=#2,label=#1,language=python]{#1.py}
\end{itemize}
}

%----------------------------------------------------------------------------------------
%	DOCUMENT STRUCTURE COMMANDS
%	Skip this unless you know what you're doing
%----------------------------------------------------------------------------------------

% Header and footer for when a page split occurs within a problem environment
\newcommand{\enterProblemHeader}[1]{
\nobreak\extramarks{#1}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
}

% Header and footer for when a page split occurs between problem environments
\newcommand{\exitProblemHeader}[1]{
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1}{}\nobreak
}

\setcounter{secnumdepth}{0} % Removes default section numbers
\newcounter{homeworkProblemCounter} % Creates a counter to keep track of the number of problems

\newcommand{\homeworkProblemName}{}
\newenvironment{homeworkProblem}[1][Problem \arabic{homeworkProblemCounter}]{ % Makes a new environment called homeworkProblem which takes 1 argument (custom name) but the default is "Problem #"
\stepcounter{homeworkProblemCounter} % Increase counter for number of problems
\renewcommand{\homeworkProblemName}{#1} % Assign \homeworkProblemName the name of the problem
\section{\homeworkProblemName} % Make a section in the document with the custom problem count
\enterProblemHeader{\homeworkProblemName} % Header and footer within the environment
}{
\exitProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

\newcommand{\problemAnswer}[1]{ % Defines the problem answer command with the content as the only argument
    \noindent\framebox[\columnwidth][c]{
        \begin{minipage}{0.98\columnwidth}
            \textbf{Answer}

            #1
        \end{minipage}
    } % Makes the box around the problem answer and puts the content inside
}

\newcommand{\homeworkSectionName}{}
\newenvironment{homeworkSection}[1]{ % New environment for sections within homework problems, takes 1 argument - the name of the section
\renewcommand{\homeworkSectionName}{#1} % Assign \homeworkSectionName to the name of the section from the environment argument
\subsection{\homeworkSectionName} % Make a subsection with the custom name of the subsection
\enterProblemHeader{\homeworkProblemName\ [\homeworkSectionName]} % Header and footer within the environment
}{
\enterProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

%----------------------------------------------------------------------------------------
%	NAME AND CLASS SECTION
%----------------------------------------------------------------------------------------

\newcommand{\hmwkTitle}{Assignment \#2} % Assignment title
\newcommand{\hmwkDueDate}{Tuesday, September 17, 2013} % Due date
\newcommand{\hmwkClass}{CSCI 8950} % Course/class
\newcommand{\hmwkClassTime}{} % Class/lecture time
\newcommand{\hmwkClassInstructor}{Dr. Rasheed} % Teacher/lecturer
\newcommand{\hmwkAuthorName}{Yuchen Ying} % Your name

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title{
\vspace{2in}
\textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
\normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
\vspace{0.1in}\large{\textit{\hmwkClassInstructor\ \hmwkClassTime}}
\vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{} % Insert date here if you want it to appear below your name

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

%\setcounter{tocdepth}{1} % Uncomment this line if you don't want subsections listed in the ToC

\newpage
\tableofcontents
\newpage

%----------------------------------------------------------------------------------------
%	PROBLEM 1
%----------------------------------------------------------------------------------------

% To have just one problem per page, simply put a \clearpage after each problem

\begin{homeworkProblem}
    For this part you will experiment with the \textit{PlayTennis} data from Table 3.2 in the textbook.

    (a) Use your decision tree learner to learn a decision tree based on all 14 instances. Print the tree if your software easily allows this or hand type it if not. Do you get the same decision tree as the one in Figure 3.1 in the textbook? If not, find out why (it may be because the learner is using an information gain measure that is different from the one the book uses).

    \problemAnswer{
        Code List \ref{code/hw2/p1} (at the end of this document) shows the code to construct the decision tree, using the \texttt{scikit-learn} Python package.

        The output graph of the tree is like this:

        \begin{center}
            \includegraphics[width=0.75\columnwidth]{code/hw2/p1.eps}
        \end{center}

        It doesn't look like the tree on the textbook. The reason might be:

        \begin{itemize}
            \item The \texttt{scikit-learn} package uses \texttt{CART} (Classification And Regression Trees) algorithm to construct the decision tree, which can only construct a binary tree, and the tree on the text book is definitely not a binary tree.
            \item The \texttt{scikit-learn} package may use a different classification criteria (difference in information gain calculation maybe).
        \end{itemize}

    }


    (b) Use the leave-one-out cross-validation method described on page 235 in the text book to estimate the error of the decision tree. You can do this manually by removing one example at a time and training on the remaining 13 and then testing on the one you removed, or you can use the package to do this for you automatically by asking for a 14-fold cross-validation if the package supports cross-validation. The See5/C5.0 package supports cross-validation.

    \problemAnswer{
        Code List \ref{code/hw2/p1} (at the end of this document) also contains the cross-validation code. The accuracy in my test was 57.14\% (8 out of 14 accuracy).
    }
\end{homeworkProblem}

\begin{homeworkProblem}
    For this part you should use a data set with at least 100 instances. You should choose one of the data sets in the UCI repository at

    \url{http://archive.ics.uci.edu/ml/}

    You may use any data set from this web page provided that you tell me which one you used! Many of the data sets in the repository have missing attribute values but See5/C5.0 can handle this automatically. For larger datasets, you may pick a subset of instances and use it instead of the full dataset but please try to use as many instances of the set as your software would allow (400 instances if you use See5/C5.0).

    (a) Use your full dataset to learn a decision tree. Give the tree, the error on the training set and the time needed for learning (if your package gives the learning time. \textbf{Do not use pruning}

    \problemAnswer{
        The dataset I'm using was \textbf{Tic-Tac-Toe Endgame Data Set} (\url{http://archive.ics.uci.edu/ml/datasets/Tic-Tac-Toe+Endgame})

        The tree was too big that I made it available online:

        \url{http://cs.uga.edu/~ying/ml/p2.png}

        Since this is a small dataset (958 lines of training examples), the learning time is less then 1 second.

        We didn't enable pruning, so the decision tree would be a full grown decision tree that accurately fit the training set. So the accuracy of this tree on the training set itself is 100\% (overfitting the training set).
    }

    (b) Use 10-fold cross validation to estimate the error (again without pruning)

    \problemAnswer{
        Code List \ref{code/hw2/p2} (at the end of this document) was used to run the 10-fold cross-validation. The score can be found in Table \ref{table_1}: the accuracy is 58.51\%.
    }

    (c) Repeat the learning on the full set and the 10-fold cross-validation with pruning allowed. You may use any pruning method you like but you should describe it. Compare in a \textbf{table} between the error on the training set and the 10-fold cross-validation with and without pruning. Comment on the time needed for learning with and without pruning (if your package gives the learning time)

    \problemAnswer{
        The python package I used to do this assignment, \texttt{scikit-learn}, doesn't support pruning on decision-tree algorithms. So I contacted Dr. Rasheed about this and he said I can do this problem using min-split. I've included the email discussion in the end of this document.

        The min-split is a number threshold when deciding a internal node of the tree needs to split into more branches. Normally when we train a full grown decision tree, min-split equals 2, which means split into branches when the internal node has at least two examples.

        I tried min-split value from 2 (which is used to train a full grow decision tree) to 10. Since the raw dataset is very small, the training time has very little difference (all of them are less then 1 second) so I just ignored the time in the table.
    }
\end{homeworkProblem}


\clearpage
\pythonscript{code/hw2/p1}{Python code to Problem 1}

\clearpage
\pythonscript{code/hw2/p2}{Python code to Problem 2}

\clearpage

\begin{table}[hbp]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        Min-split & Accuracy \\
        \hline
        2 & 0.585197368421 \\
        \hline
        3 & 0.582072368421 \\
        \hline
        4 & 0.603958333333 \\
        \hline
        5 & 0.583157894737 \\
        \hline
        6 & 0.58836622807 \\
        \hline
        7 & 0.580032894737 \\
        \hline
        8 & 0.56961622807 \\
        \hline
        9 & 0.57274122807 \\
        \hline
        10 & 0.56024122807 \\
        \hline
    \end{tabular}
    \caption{10-fold cross-validation score with different min-split value}
    \label{table_1}
\end{table}

\clearpage
\begin{verbatim}
    Hi Yuchen,

    Sure. The purpose of pruning was to cure the overfitting problem.
    Preventing overfitting using the min-split idea is fine too. That would be
    acceptable. Please print this email or write a note and include it in your
    homework.

    Regards,
    Khaled
    ****************************************
    Khaled M. Rasheed
    Associate Professor of Computer Science
    AI Institute Graduate Coordinator
    The University of Georgia
    Phone:(706)542-3444
    khaled@cs.uga.edu              http://www.cs.uga.edu/~khaled

    From: Yuchen Ying
    Sent: Sunday, September 15, 2013 3:35 AM
    To: Khaled M Rasheed
    Subject: Question about decision tree pruning

    Dr. Rasheed,

    I'm using the python package scikit-learn to do my homework assignment 2.
    This is a powerful package but unfortunately it doesn't support decision
    tree pruning. I dig into the mailing list archive and one of the authors
    said [1] the pruning is not very practical in production thus this feature
    wasn't implemented.

    I was wondering if I can do problem \#2 of this assignment in a different
    way, that I'll train multiple trees using different min\_sample\_split
    (which is the threshold whether the algorithm should continue splitting the
    training set) and compare the performance.

    Thank you.

    [1]: http://osdir.com/ml/python-scikit-learn/2011-12/msg00023.html 

    --
    Yuchen Ying
    Graduate Student, Computer Science
    University of Georgia
    yegle@uga.edu
    http://cs.uga.edu/~ying
\end{verbatim}

\end{document}
